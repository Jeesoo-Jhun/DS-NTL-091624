{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeesoo-Jhun/DS-NTL-091624/blob/main/%5BFIS_DS%5D_CHALLENGE_Guided_MLR_into_Pima_Indian_Diabetic_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "ooMVvHZk5H8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§° **CHALLENGE ASSIGNMENT** ðŸ§°\n",
        "\n",
        "## **A machine-learning-driven sample investigation into diabetic predictions on individuals of Pima Indian descent.**"
      ],
      "metadata": {
        "id": "e6NkH33k5KfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this challenge, you'll perform an end-to-end data analysis and machine learning summary assessment with a new dataset that's well-known in the predictive analytics world: the **[Pima Indians Diabetes](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)** dataset.\n",
        "\n",
        "For the sake of this investigation, the dataset will be referred to programmatically via the filename **`diabetes.csv`**."
      ],
      "metadata": {
        "id": "Q-LQC45QVrZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "nxLOmkofNwtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As this is an end-to-end machine learning analysis, we'll be performing sufficient initial data cleaning, investigation, and summarization in order to get the best sense for how to best construct effective models."
      ],
      "metadata": {
        "id": "WRu5TDYaNxhA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUfwl2FMhGeD"
      },
      "outputs": [],
      "source": [
        "# System and Operational Importations\n",
        "import sys, os\n",
        "\n",
        "# Standard Data Science/Analysis Toolkit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt; plt.style.use(\"ggplot\")\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning Tools, Utilities, and Scoring Metrics\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "# Suite of Machine Learning Algorithms\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "# Setup to Ignore Version Errors and Deprecations\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For starters, let's get access to our dataset.\n",
        "\n",
        "At this point, the dataset should be externally uploaded to your current local session using the relevant toolbar to the left-hand-side of this window.\n",
        "\n",
        "Assuming that step is complete, you can run the following cell to import our data."
      ],
      "metadata": {
        "id": "1si1hThrRXfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"diabetes.csv\"\n",
        "\n",
        "dataset = pd.read_csv(PATH)"
      ],
      "metadata": {
        "id": "5_zPAkgsRZY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've gotten access to your dataset, you can investigate general information on its features and overall shape."
      ],
      "metadata": {
        "id": "_lMLxeO_J_yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "id": "t8BkI9FgRjYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, you can use a tried-and-tested method for understanding a sample of the data: **`.head()`**."
      ],
      "metadata": {
        "id": "sKZXtLw7PP5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head(3)"
      ],
      "metadata": {
        "id": "qR1tlXpbRrWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also get some basic descriptive statistics across the dataset using the **`.describe()`** method."
      ],
      "metadata": {
        "id": "cVPes-FIPuKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.describe()"
      ],
      "metadata": {
        "id": "sptajOjHRs_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To progress further into the investigation, null value occurrences across our data need to be cleaned and imputed.\n",
        "\n",
        "For this data, however, as you probably recall from our **`.info()`** call up above, there doesn't appear to be any occurrences of **`np.nan`** values.\n",
        "\n",
        "Does this mean that there truly are no null values at all?\n",
        "\n",
        "Well, not exactly.\n",
        "\n",
        "In this dataset, there appear to be several zero-based values occur in features that don't make logical sense.\n",
        "\n",
        "For instance, assuming **`BloodPressure`** measures a patient's blood pressure (which I'd say is a more-than-fair assumption), a blood pressure of zero (0) makes no sense for this type of dataset.\n",
        "\n",
        "This can only mean that the patient information was likely not logged during data entry and that the data was pre-imputed with zero-values.\n",
        "\n",
        "We can see this pattern emerge across five features in particular:\n",
        "- _Patient Glucose Level_ (**`Glucose`**)\n",
        "- _Patient Blood Pressure_ (**`BloodPressure`**)\n",
        "- _Patient Skin Thickness_ (**`SkinThickness`**)\n",
        "- _Patient Insulin Level_ (**`Insulin`**)\n",
        "- _Patient Body Mass Index_ (**`BMI`**)\n",
        "\n",
        "These features are all similar in that they comprise values where zero-value occurrences don't make logical sense in terms of the data's domain, as those values cannot exist in human conditions and therefore can be permissibly interpreted as data entry oversights that are likely due to null value recording.\n",
        "\n",
        "As such, you'll need to analyze zero-value occurrences in these five features and convert them _back_ into true null values (**`np.nan`**) before moving ahead with appropriate null value imputation.\n",
        "\n",
        "For starters, get a basic assessment of all the summed occurrences of null values across the dataset by feature by converting zero-value occurrences in the previously-detected zero-invalid features and then counting **`np.nan`** occurrences."
      ],
      "metadata": {
        "id": "zco9Z1sBP3Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INVALID_NULL_FEATURES = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n",
        "\n",
        "try:\n",
        "    dataset[INVALID_NULL_FEATURES] = dataset[INVALID_NULL_FEATURES].replace(0, np.NaN)\n",
        "except dataset[INVALID_NULL_FEATURES].isnull().sum().sum() == 0:\n",
        "    pass\n",
        "finally:\n",
        "    print(dataset.isnull().sum())"
      ],
      "metadata": {
        "id": "9ki5cN_kngwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There appears to be far more null values propagated across this dataset than originally detected!\n",
        "\n",
        "At least they're in the correct format now.\n",
        "\n",
        "Before beginning to truly impute them, go ahead and run the following cell to produce an effective data visualization showcasing the distributions of each major feature in the dataset."
      ],
      "metadata": {
        "id": "HrSbKH0TTT4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.hist(rwidth=0.95, figsize=(12, 12))"
      ],
      "metadata": {
        "id": "XE87mTa5YBch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that for the invalid features, there exist presences of zero-value occurrences (towards the left-hand side of their respective visualizations).\n",
        "\n",
        "Keep this in mind as you perform some advanced data imputation for the aforementioned features in an attempt to sanitize the data.\n",
        "\n",
        "Specifically, you'll impute the features based on the occurrences of normal distributions in their data."
      ],
      "metadata": {
        "id": "gNExcxEiIWeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "> ### ðŸ“Œ **OBJECTIVE: Distribution-Based Null Feature Imputation** ðŸ“Œ\n",
        ">\n",
        "> Generally in previous courses and work, you've accomplished basic null value imputation by simply removing occurrences of null values as they occur or, at the very most, replacing them with manually-assigned \"dummy values\" (e.g. -1, 0, \"NULL\").\n",
        ">\n",
        "> However, null value imputation is a fairly comprehensive subdomain of data analysis and can necessitate more advanced methods as needed to retain as much signal as possible, especially when performing more advanced tasks overall such as those in the context of machine learning.\n",
        ">\n",
        "> ðŸ” _For this objective, your task is to perform mean-based and median-based null value imputation on the aforementioned five zero-invalid features._ ðŸ”Ž\n",
        ">\n",
        "> #### **The major idea to understand is that data that is _normally distributed_ can be imputed with that feature's _mean_, while data that is _non-normally distributed_ or substantially skewed can be imputed with that feature's _median_.**\n",
        ">\n",
        "> In other words, while much of the actual null value imputation code is pre-written for you - you'll have to consider carefully which of the five zero-invalid features are normally distributed vs. non-normally distributed/skewed in order to discern which features should be imputed by mean or by median.\n",
        ">\n",
        "> Consider your choices carefully and feel free to experiment with different combinations and selections: your choices of how to impute specific features may very well have an impact on the final performances of your models!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WIVLd-fsaW5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INVALID_NULL_FEATURES_NORMAL =      [\"\"\" EDIT ME! \"\"\"]\n",
        "INVALID_NULL_FEATURES_NONNORMAL =   [\"\"\" EDIT ME! \"\"\"]\n",
        "\n",
        "try:\n",
        "    for feature_normal in INVALID_NULL_FEATURES_NORMAL:\n",
        "        dataset[feature_normal].fillna(dataset[feature_normal].mean(),\n",
        "                                       inplace=True)\n",
        "    for feature_nonnormal in INVALID_NULL_FEATURES_NONNORMAL:\n",
        "        dataset[feature_nonnormal].fillna(dataset[feature_nonnormal].median(),\n",
        "                                          inplace=True)\n",
        "except dataset[INVALID_NULL_FEATURES].isnull().sum().sum() == 0:\n",
        "    pass\n",
        "finally:\n",
        "    print(dataset.isnull().sum())"
      ],
      "metadata": {
        "id": "g4zyRtITY9Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once your imputation task is completed, you can visualize the resultant distributions by running the code below.\n",
        "\n",
        "You'll notice it's a replica of the original feature-wise data distribution visualization code -- the difference is that now you should be able to see slight alterations in the zero-invalid features now that they've been appropriately imputed with mean-based or median-based replacement methods."
      ],
      "metadata": {
        "id": "vy_lYt6hdTxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.hist(rwidth=0.95, figsize=(12, 12))"
      ],
      "metadata": {
        "id": "WXzOGrkIaisx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a final sanity check for this analysis, you can also distribute your data in another feature-wise method with respect to each feature's general impactfulness on the target.\n",
        "\n",
        "In this case, the dataset's target feature is measured as **`Outcome`**, so the relevant feature-wise plot can be constructed as such."
      ],
      "metadata": {
        "id": "tSfcxt2LdpV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(dataset, hue=\"Outcome\", palette=\"husl\")"
      ],
      "metadata": {
        "id": "lrtE8gsVRu3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And lastly, you can produce a heatmap to measure correlational values between each pair of features.\n",
        "\n",
        "This is extremely useful to build an intuition for basic feature selection or reduction, as well as to gain a broad understanding of where signal is distributed across the data."
      ],
      "metadata": {
        "id": "3k8v4OFId-Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "sns.heatmap(dataset.corr(), annot=True, cmap=\"RdYlGn\")"
      ],
      "metadata": {
        "id": "ABHfo_qmnO6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that relevant predecessory data analysis and visualization has been conducted successfully, you're ready to move on to machine learning and finally developing your algorithmic sample platter!"
      ],
      "metadata": {
        "id": "njiXHhJVeVYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "LCVknioLb5Bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For starters, as with every machine learning analysis, you start by identifying the relevant target variable across the dataset and isolating it in order to produce **`X`** and **`y`** data segments."
      ],
      "metadata": {
        "id": "02_5JhBCeq0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET = [\"Outcome\"]\n",
        "\n",
        "X, y = dataset.drop(columns=TARGET, axis=1), dataset[TARGET]"
      ],
      "metadata": {
        "id": "zXAZQnkQb-n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the data has been segmented, generally it's acceptable to move directly on to further segmentation into training and testing splits.\n",
        "\n",
        "However, it's good practice when conducting machine learning to scale and normalize the data prior to predictive modeling -- especially when working with some types of classifiers such as ones that use distance-based metrics (e.g. KNN, SVM).\n",
        "\n",
        "As such, you'll make use of the **`StandardScaler()`** module to scale the data prior to further segmentation and fitness.\n",
        "\n",
        "To start, you'll instantiate and declare the relevant data structure."
      ],
      "metadata": {
        "id": "qBCGvWAsfvWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "2qEZ3_fCb5--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the relevant data structure is instantiated, you can fit and transform it against the independent data (**`X`**).\n",
        "\n",
        "This can be done in two explicit lines of code (one calling **`.fit()`** and one calling **`.transform()`**) but for brevity's sake, it can also be done and is recommended to be performed in one hybridized call of **`.fit_transform()`**."
      ],
      "metadata": {
        "id": "eTBb4VFZgSsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "aC98cKm5cKQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the last cell produces a new copy of the data that retains scaled changes, referred to as **`X_scaled`**.\n",
        "\n",
        "This is good practice when performing post-analysis dataset alterations: to persist major changes across copies of the originally processed data such that it's easier to track how said changes impact modeling performance.\n",
        "\n",
        "It's important to make sure that references to the independent data are updated across future steps of machine learning analysis to reflect correctly across the newly scaled data rather than the original copy.\n",
        "\n",
        "As such, the subsequent training/testing segmentation call will reference the newer copy **`X_scaled`** as opposed to the original."
      ],
      "metadata": {
        "id": "k1bC7Yfzgj8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y,\n",
        "                                                                  train_size=0.7,\n",
        "                                                                  test_size=0.3,\n",
        "                                                                  random_state=42)"
      ],
      "metadata": {
        "id": "X_jfZH4yckoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this specific assessment, a 70/30 split for training and testing data is used for simplicity's sake -- however, this specific split is a parameter that can be manipulated for performance optimization."
      ],
      "metadata": {
        "id": "LB1dD8D1hGCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "> ### ðŸ“Œ **OBJECTIVE: Alternate Scaling Methods** ðŸ“Œ\n",
        ">\n",
        "> The object that's been in use so far for scaling data (**`StandardScaler()`**) is a powerful one, but only one in a competent data scientist's arsenal.\n",
        ">\n",
        "> The naming convention behind the data structure reveals its secrets: it utilizes normalization (also referred to as _standardization_) to retrofit data within a feature such that while its distribution's shape is not changed, the data is altered such that the mean of the data falls on zero (0) and the standard deviation of the data falls on one (1).\n",
        ">\n",
        "> In this way, all features in a dataset can be reincorporated within a similar domain to improve performance without sacrificing the nuance and expressed variance of each unique feature.\n",
        ">\n",
        "> Another popular scaling method exists, however, that can often perform more optimally than the standard scaler due to how important boundaries can make an impact on predictive performance.\n",
        ">\n",
        "> #### **That data structure is the `MinMaxScaler()` and its naming convention reveals that it transfigures all data based on their minimum and maximum values without impacting the general shape of their distributions.**\n",
        ">\n",
        "> ðŸ” _Your task is to perform a second scaling step across the original data by utilizing **`MinMaxScaler()`** instead of `StandardScaler()` to fit and transform the data._ ðŸ”Ž\n",
        ">\n",
        "> Remember to name your transformed `X` data relevantly upon performing this alternate scaling method!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JR2n0mJCksEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Complete OBJECTIVE: Alternate Scaling Methods in this cell! \"\"\""
      ],
      "metadata": {
        "id": "0F_mo5fFmsS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our data is relevantly scaled and formatted for machine learning, it's time to set up the relevant predictive algorithms!\n",
        "\n",
        "For this challenge, it's important to get used to working with multiple classifiers at once -- oftentimes in the context of real-world machine learning, one never relies on simply one or two algorithms all the time to complete their investigations.\n",
        "\n",
        "Indeed, it's often more effective to leverage a suite of algorithms in order to identify the most performant ones as well as identify more interesting patterns and heuristics of the data based on how well specific algorithms fit to them.\n",
        "\n",
        "In this specific challenge, five major algorithms are leveraged:\n",
        "\n",
        "- **K-Nearest Neighbors** (`KNeighborsClassifier()`)\n",
        "- **Support Vector Machine** (`SVC()`)\n",
        "- **CART-Based Decision Tree** (`DecisionTreeClassifier()`)\n",
        "- **Gaussian Naive Bayes** (`GaussianNB()`)\n",
        "- **Logistic Regressor** (`LogisticRegression()`)"
      ],
      "metadata": {
        "id": "Dy9cU0__ksSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"KNN\": {\n",
        "        \"Estimator\": KNeighborsClassifier(),\n",
        "        },\n",
        "    \"SVM\": {\n",
        "        \"Estimator\": SVC(),\n",
        "        },\n",
        "    \"CART\": {\n",
        "        \"Estimator\": DecisionTreeClassifier(),\n",
        "        },\n",
        "    \"NB\": {\n",
        "        \"Estimator\": GaussianNB(),\n",
        "        },\n",
        "    \"LOGREG\": {\n",
        "        \"Estimator\": LogisticRegression(),\n",
        "        }\n",
        "}\n",
        "\n",
        "for name in models:\n",
        "    models[name][\"All_Scores\"] = list()\n",
        "    models[name][\"Top_Score\"] = float()\n",
        "    models[name][\"Mean_Score\"] = float()\n",
        "    models[name][\"Std_Score\"] = float()"
      ],
      "metadata": {
        "id": "bQQLH5cSdA1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this challenge, the **`models`** object will be used to reference not just the actual algorithmic data structures themselves, but also their performance metrics per each assessment walkthrough.\n",
        "\n",
        "In other words, this data structure will represent your sample platter.\n",
        "\n",
        "In order to capture the most reliable accuracy determinations per assessment, you can leverage _K-Folds Cross Validation_ with enough folds to obtain a healthily generalized performance score for each model.\n",
        "\n",
        "For the sake of simplicity, ten (10) folds will be used.\n",
        "\n",
        "Because of this, four values will be ascertained and saved from each simulation:\n",
        "\n",
        "- **All Ten (10) Scores per Each Fold** (`models[name][\"All_Scores\"]`)\n",
        "- **The Top Score from All Folds** (`models[name][\"Top_Score\"]`)\n",
        "- **The Average Score from All Folds** (`models[name][\"Mean_Score\"]`)\n",
        "- **The Standard Deviation of Scores Across All Folds** (`models[name][\"Std_Score\"]`)\n",
        "\n",
        "By running the next cell, you should be able to populate the `models` data structure with relevant values for all folds of cross-validation and obtain the aforementioned four values."
      ],
      "metadata": {
        "id": "Zf5nFAc-nuI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_FOLDS, SCORING = 10, \"accuracy\"\n",
        "\n",
        "for name in models:\n",
        "    folds = StratifiedKFold(n_splits=NUM_FOLDS)\n",
        "    results = cross_val_score(estimator=models[name][\"Estimator\"],\n",
        "                              X=X_train_scaled,\n",
        "                              y=y_train,\n",
        "                              cv=folds,\n",
        "                              scoring=SCORING)\n",
        "    models[name][\"Top_Score\"] = results.max()\n",
        "    models[name][\"Mean_Score\"] = results.mean()\n",
        "    models[name][\"Std_Score\"] = results.std()\n",
        "    for result in results:\n",
        "        models[name][\"All_Scores\"].append(result)"
      ],
      "metadata": {
        "id": "jqn6lz95ksmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that your model-tracking metrics data structure is effectively populated, you can go ahead and take a look at your best performant algorithms using the cell below."
      ],
      "metadata": {
        "id": "FblItnC5o6lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name in models:\n",
        "    print(\"\\n[MODEL TYPE: {}]\\n\".format(name))\n",
        "    print(\">>>> Top Performance: \\t\\t{:.4f}\".format(models[name][\"Top_Score\"]))\n",
        "    print(\">>>> Average Performance: \\t{:.4f}\".format(models[name][\"Mean_Score\"]))\n",
        "    print(\">>>> Spread of Performance: \\t{:.4f}\".format(models[name][\"Std_Score\"]))"
      ],
      "metadata": {
        "id": "mDwsK43uFogU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "> ### ðŸ“Œ **OBJECTIVES: Picking and Fine-Tuning \"The Best Dish\"** ðŸ“Œ\n",
        ">\n",
        "> From the last cell, you should have obtained metrics for the five tested models.\n",
        ">\n",
        "> But your work isn't done!\n",
        ">\n",
        "> ðŸ” _Now, you have the task of selecting the top performant model from the prior five and performing some model optimization and evaluation techniques in order to try and get the accuracy scores as high as you can!_ ðŸ”Ž\n",
        ">\n",
        ">\n",
        "> Remember, there's several ways of doing this:\n",
        ">\n",
        "> - 1ï¸âƒ£ **Hyperparameter Tuning for the Algorithm** ðŸ”¬\n",
        "> - 2ï¸âƒ£ **Implementing Dimensionality Reduction with PCA/LDA** ðŸ”³\n",
        "> - 3ï¸âƒ£ **Additional Methods for Data Cleaning and Preprocessing** ðŸ§½\n",
        "> - 4ï¸âƒ£ **Tweaking Values for Cross-Validation and Train/Test Splitting** ðŸ•\n",
        "> - 5ï¸âƒ£ **New Variants of the Predictive Classifiers** ðŸ“š\n",
        ">\n",
        "> The true character of a data scientist is not how they start: it's how they finish.\n",
        ">\n",
        "> ðŸ” _As such, you're now required to perform **at least four (4)** of these five model improvement steps using the skills you've covered in previous seminars and tutorials._ ðŸ”Ž\n",
        ">\n",
        "> For example, you can choose to perform **hyperparameter tuning**, **dimensionality reduction**, **tweaking values**, and **testing new classifier variants** in order to satisfy the remaining objectives in this notebook.\n",
        ">\n",
        "> #### **You're recommended to produce these four changes in a new machine learning analysis using cells below this text as opposed to altering the cells above -- this way, you can visually see the performance improvements and difference in accuracy in your top selected model.**\n",
        ">\n",
        "> This is a much more extensive and more exhaustive set-of-objectives than any other objective in the notebook and will represent the conclusion of this assignment -- spend as much time/effort on this component as you can to ensure successful improvement to your modeling accuracies.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-QUkso8-pP0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Complete OBJECTIVES: Picking and Fine-Tuning 'The Best Dish'\n",
        "Using This Cell & Any Additional Cells Needed!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "y7WM0bglsnhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've completed the final objective and are satisfied with your progression in this notebook, consider this challenge completed!\n",
        "\n",
        "Great work!"
      ],
      "metadata": {
        "id": "dZjjSJ06rWLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "a-UJQASC5JHC"
      }
    }
  ]
}